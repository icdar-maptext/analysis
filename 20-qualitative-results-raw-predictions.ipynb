{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images to investigate the predictions from each approach, without considering evaluation\n",
    "The idea is to produce a first, easier, set of images before taking more dev time to produce detailed visualization which re-run the evaluation selectively.\n",
    "\n",
    "We will try to compare predictions between methods and against the ground truth, for each task.\n",
    "\n",
    "We also want to be able to visualize:\n",
    "\n",
    "- best results (different for every method **OR, better, samples easy for all methods**)\n",
    "- worst results (different for every method **OR, better, samples hard for all methods**)\n",
    "- random results, with fixed seed (same input for every method, for comparison)\n",
    "\n",
    "For the RUMSEY subset, loading the predictions can be challenging because of the size of some files, the code developed for the IGN subset must be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check we have all images from test sets available.\n",
    "\n",
    "Note: generation of figures with images may be extracted to another notebook for clarity and to avoid the need to download image test sets for basic graph generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.loaders import check_for_missing_images, VALID_SUBSETS\n",
    "print(\"Valid subsets:\", VALID_SUBSETS)\n",
    "for subset in VALID_SUBSETS:\n",
    "    if len(missing_image := check_for_missing_images(subset)) > 0:\n",
    "        print(f\"Missing images in {subset}: {missing_image}\")\n",
    "        raise RuntimeError(f\"Missing images in {subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select some random images for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.loaders import list_gt_images, TypeDatasetName\n",
    "\n",
    "def select_random_images(subset: TypeDatasetName, n: int, seed: int = 42) -> list[str]:\n",
    "    images_list = list_gt_images(subset)\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(images_list)\n",
    "    return images_list[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the list of valid submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.loaders import load_valid_submissions_metadata\n",
    "\n",
    "valid_submissions_metadata = load_valid_submissions_metadata()\n",
    "valid_submissions_metadata.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.loaders import TypeTaskId\n",
    "import pandas as pd\n",
    "def select_valid_submissions(task_id: TypeTaskId, subset: TypeDatasetName) -> pd.DataFrame:\n",
    "    return valid_submissions_metadata[(valid_submissions_metadata.task == task_id) & (valid_submissions_metadata.subset == subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_valid_submissions(1, \"rumsey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display raw predictions for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a submission has the following format:\n",
    "# [ # Begin a list of images\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME1\",\n",
    "#      \"groups\": [ # Begin a list of phrase groups for the image\n",
    "#         [ # Begin a list of words for the phrase\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT1\"},\n",
    "#           ...,\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT2\"}\n",
    "#        ],\n",
    "#        ...\n",
    "#        [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT3}, ... ]\n",
    "#     ] },\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME2\",\n",
    "#      \"groups\": [\n",
    "#         [\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT4\"},\n",
    "#           ...,\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT5\"}\n",
    "#         ],\n",
    "#         ...\n",
    "#         [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT6\"}, ... ] \n",
    "#     ] },\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ground truth has the following format:\n",
    "# [ # Begin a list of images\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME1\",\n",
    "#      \"groups\": [ # Begin a list of phrase groups for the image\n",
    "#          [  # Begin a list of words for the phrase\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT1\", \"illegible\": False, \"truncated\": False},\n",
    "#            ...,\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT2\", \"illegible\": True, \"truncated\": False}\n",
    "#          ],\n",
    "#           ...\n",
    "#          [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT3\", \"illegible\": False, \"truncated\": True}, ... ]\n",
    "#      ] },\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME2\",\n",
    "#      \"groups\": [\n",
    "#          [\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT4\", \"illegible\": False, \"truncated\": False},\n",
    "#            ...,\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT5\", \"illegible\": False, \"truncated\": False}],\n",
    "#           ...\n",
    "#          [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT6\", \"illegible\": False, \"truncated\": False}, ... ] \n",
    "#      ] },\n",
    "#      ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detection_for_image(submission: list[dict], image_id: str) -> list[list]:\n",
    "    \"\"\"Extract predictions for a given image from  a submission or gt\n",
    "\n",
    "        [ [GROUP1], [GROUP2], ... ],\n",
    "    \"\"\"\n",
    "    for image_data in submission:\n",
    "        image_name = image_data[\"image\"]\n",
    "        if image_name == image_id:\n",
    "            return image_data[\"groups\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.loaders import open_image, load_gt, load_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_comparison_detection(gt_data_image: list[list[dict]], submission_data_image: list[list[dict]], image_id: str, submission_name=None, ax=None):\n",
    "    # TODO load the image in the caller\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # plot the ground truth\n",
    "    for group in gt_data_image:\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"red\" if (word[\"illegible\"] or word[\"truncated\"]) else \"green\"\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "\n",
    "    # plot the submission\n",
    "    if submission_data_image is not None:\n",
    "        for group in submission_data_image:\n",
    "            for word in group:\n",
    "                vertices = np.array(word[\"vertices\"])\n",
    "                ax.add_patch(patches.Polygon(vertices, edgecolor=\"blue\", facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "    \n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    # title = f\"Predictions of {submission_name}\" if submission_name else \"Predictions\"\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    title += f\" vs GT\" #\\n{image_id}\"\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar_maptext_analysis.submissions_metadata import shorten_title\n",
    "\n",
    "def compare_submissions_to_gt_task1(subset: str, image_id: str, filename: str = None, use_two_rows: bool = False):\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(1, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    _fig, axs = None, None\n",
    "    ax_row_len = None\n",
    "    if use_two_rows:\n",
    "        ax_row_len = (len(valid_submissions)+1)//2\n",
    "        _fig, axs = plt.subplots(2, ax_row_len, figsize=(5*ax_row_len, 5*2))\n",
    "    else:  # Use only 1 row\n",
    "        _fig, axs = plt.subplots(1, len(valid_submissions), figsize=(5*len(valid_submissions), 5))\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(1, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        plot_ax = axs[plot_id] if not use_two_rows else axs[plot_id//ax_row_len, plot_id%ax_row_len]\n",
    "        display_image_comparison_detection(gt_data_image, submission_data_image, image_id, submission_name=submission_name, ax=plot_ax)\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "\n",
    "        # enlarge text if we use two rows\n",
    "        if use_two_rows:\n",
    "            plot_ax.title.set_fontsize(20)\n",
    "\n",
    "    # add some extra vertical space between the two rows\n",
    "    if use_two_rows:\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # remove axis labels\n",
    "    if use_two_rows:\n",
    "        for ax in axs:\n",
    "            for axx in ax:\n",
    "                axx.axis(\"off\")\n",
    "                \n",
    "    else:\n",
    "        for ax in axs:\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average quality for each image over the submissions for a given task and subset\n",
    "from icdar_maptext_analysis.loaders import load_evaluations\n",
    "def compute_average_quality(task_id: int, subset: str) -> pd.Series:\n",
    "    valid_submission_ids = select_valid_submissions(task_id, subset).submission_id.to_list()\n",
    "    _results_global, results_per_image = load_evaluations(task_id, subset, filter_fn=lambda x: int(x) in valid_submission_ids)\n",
    "    criterion = \"hmean\"\n",
    "    return results_per_image[criterion].groupby(by=\"image_id\").mean().sort_values(ascending=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import os\n",
    "\n",
    "def generate_plot_raw_predictions(taskid: int, output_dir:str, display_function: Callable, num_images: int=10):\n",
    "    \"\"\"\n",
    "    display_function: (subset, image_id, filename) -> None\n",
    "    \"\"\"\n",
    "    output_dir = f\"{output_dir}/task{taskid}\"\n",
    "    # random images\n",
    "    for subset in reversed(VALID_SUBSETS):\n",
    "        out_dir = f\"{output_dir}/{subset}/random\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        random_images_subset = select_random_images(subset, num_images)\n",
    "        for ii, image_id in enumerate(random_images_subset):\n",
    "            print(f\"Generating plot for task {taskid}/{subset} {image_id} (random#{ii})\")\n",
    "            image_name = os.path.splitext(os.path.basename(image_id))[0]\n",
    "            display_function(subset, image_id, filename=f\"{out_dir}/{image_name}.pdf\")\n",
    "\n",
    "    # easy and hard images for current task\n",
    "    for subset in reversed(VALID_SUBSETS):\n",
    "        sample_sorted_values = compute_average_quality(taskid, subset)\n",
    "        easy_images = sample_sorted_values.head(num_images).index.to_list()\n",
    "        hard_images = sample_sorted_values.tail(num_images).index.to_list()\n",
    "        # generate comparison plots for the easy and hard images\n",
    "        for difficulty_name, difficulty_group in [\n",
    "            (\"easy\", easy_images), \n",
    "            (\"hard\", hard_images),\n",
    "            ]:\n",
    "            for ii, image_id in enumerate(difficulty_group):\n",
    "                print(f\"Generating plot for task {taskid}/{subset} {image_id} ({difficulty_name}#{ii})\")\n",
    "                image_name = os.path.splitext(os.path.basename(image_id))[0]\n",
    "                out_dir = f\"{output_dir}/{subset}/{difficulty_name}\"\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                display_function(subset, image_id, filename=f\"{out_dir}/{image_name}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and create the output directory\n",
    "OUTPUT_DIR_BASE = \"data/20-raw-predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results for task 1\n",
    "generate_plot_raw_predictions(1, OUTPUT_DIR_BASE, compare_submissions_to_gt_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display raw predictions for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Optional reference of qualitative maps\n",
    "# Paired: 12, Dark2: 8, Set1: 9, tab10: 10, tab20: 20\n",
    "# _SUPPORTED_QUALITATIVE = {\"Paired\", \"Dark2\", \"Set1\", \"tab10\", \"tab20\"}\n",
    "\n",
    "# Okabe–Ito colorblind-safe palette (RGB in [0,1])\n",
    "_OKABE_ITO = [\n",
    "    (0.0, 0.45, 0.70),   # blue\n",
    "    (0.87, 0.80, 0.10),  # yellow\n",
    "    (0.0, 0.62, 0.45),   # green\n",
    "    (0.94, 0.40, 0.0),   # vermilion\n",
    "    (0.80, 0.47, 0.74),  # purple\n",
    "    (0.35, 0.70, 0.90),  # sky blue\n",
    "    (0.0, 0.0, 0.0),     # black\n",
    "    (0.90, 0.60, 0.0),   # orange\n",
    "]\n",
    "\n",
    "def generate_polygon_colors(\n",
    "    k: int,\n",
    "    *,\n",
    "    cmap: Optional[str] = None,\n",
    "    colorblind_safe: bool = False\n",
    ") -> List[Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "    Return up to 20 *unique* colors with strong discriminability for polygon fills.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of polygons visible at once (max concurrent, not total possible types).\n",
    "    cmap : str, optional\n",
    "        Name of a qualitative matplotlib colormap to force (ignored if colorblind_safe=True).\n",
    "        If None and colorblind_safe=False, auto-selects:\n",
    "            - \"Paired\" if k <= 12\n",
    "            - \"tab20\" if 13 <= k (capped to 20)\n",
    "    colorblind_safe : bool, default=False\n",
    "        If True, returns the Okabe-Ito colorblind-friendly palette\n",
    "        (length = min(k, 8)), no repeats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[float, float, float]]\n",
    "        Ordered list of RGB tuples in [0, 1], with no duplicates.\n",
    "        Length = min(k, available_colors_in_map, 20).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - No shuffling is performed; order is stable across calls.\n",
    "    - For k > available_colors_in_map, result is capped (no duplicates).\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        return []\n",
    "\n",
    "    # Colorblind-safe mode overrides all else\n",
    "    if colorblind_safe:\n",
    "        return _OKABE_ITO[:min(k, len(_OKABE_ITO))]\n",
    "\n",
    "    # Auto-select qualitative map if none provided\n",
    "    if cmap is None:\n",
    "        cmap = \"Paired\" if k <= 12 else \"tab20\"\n",
    "\n",
    "    cm = plt.get_cmap(cmap)\n",
    "\n",
    "    # Prefer discrete color lists from qualitative maps\n",
    "    if hasattr(cm, \"colors\"):\n",
    "        base = [tuple(c[:3]) for c in cm.colors]  # strip alpha if present\n",
    "    else:\n",
    "        # Fallback: sample evenly from continuous map\n",
    "        target = min(k, 20)\n",
    "        if target == 1:\n",
    "            base = [tuple(cm(0.5)[:3])]\n",
    "        else:\n",
    "            base = [tuple(cm(i / (target - 1))[:3]) for i in range(target)]\n",
    "\n",
    "    # Enforce uniqueness\n",
    "    unique: List[Tuple[float, float, float]] = []\n",
    "    seen = set()\n",
    "    for col in base:\n",
    "        if col not in seen:\n",
    "            unique.append(col)\n",
    "            seen.add(col)\n",
    "\n",
    "    return unique[:min(k, 20, len(unique))]\n",
    "\n",
    "\n",
    "\n",
    "# Example:\n",
    "# K = len(polygons_visible_now)\n",
    "# COLORS = generate_polygon_colors(K)            # auto-selects Paired/tab20\n",
    "# COLORS = generate_polygon_colors(K, cmap=\"Set1\")  # force a specific palette\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Simulate number of polygons visible now\n",
    "K = 28\n",
    "COLORS = generate_polygon_colors(K)\n",
    "\n",
    "# Plot preview\n",
    "fig, ax = plt.subplots(figsize=(10, 1))\n",
    "for i, color in enumerate(COLORS):\n",
    "    ax.add_patch(patches.Rectangle((i, 0), 1, 1, facecolor=color, edgecolor=\"none\"))\n",
    "ax.set_xlim(0, len(COLORS))\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_groups(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # load the image\n",
    "    # if submission_name is not None:\n",
    "    #     print(f\"Displaying image {image_id} for submission {submission_name}\")\n",
    "    # else:\n",
    "    #     print(f\"Displaying image {image_id}\")\n",
    "\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    COLORS = generate_polygon_colors(len(det_image))\n",
    "    # plot the groups\n",
    "    for ii, group in enumerate(det_image):\n",
    "        # pick a color for the group\n",
    "        color = COLORS[ii % len(COLORS)]\n",
    "        # draw boxes for each word in the group\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "        # draw lines between the words in the group\n",
    "        for i in range(1, len(group)):\n",
    "            vertices1 = np.array(group[i-1][\"vertices\"])\n",
    "            center1 = np.mean(vertices1, axis=0)\n",
    "            vertices2 = np.array(group[i][\"vertices\"])\n",
    "            center2 = np.mean(vertices2, axis=0)\n",
    "            ax.plot([center1[0], center2[0]], [center1[1], center2[1]], color=color, linewidth=2, alpha=0.8)\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gouping_comparison_task2(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 2\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "    # Ensure axs is always iterable\n",
    "    if len(valid_submissions)+1 == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_image_groups(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_image_groups(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(2, OUTPUT_DIR_BASE, plot_gouping_comparison_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Asian characters, we need to have the Noto CJK fonts installed:\n",
    "# This can be done on Ubuntu with:\n",
    "# $ sudo apt install fonts-noto-cjk\n",
    "\n",
    "from matplotlib import font_manager\n",
    "\n",
    "def get_font_with_cjk_fallback():\n",
    "    \"\"\"\n",
    "    Returns a FontProperties object for a font that supports both\n",
    "    Chinese and Latin characters.\n",
    "    Prefers Noto Sans CJK, falls back to common alternatives.\n",
    "    \"\"\"\n",
    "    # Ordered list of preferred fonts\n",
    "    preferred_fonts = [\n",
    "        \"Noto Sans CJK SC\",    # Simplified Chinese\n",
    "        \"Noto Sans CJK TC\",    # Traditional Chinese\n",
    "        \"Noto Sans CJK JP\",    # Japanese (also supports Chinese)\n",
    "        \"WenQuanYi Zen Hei\",   # Common in some Linux distros\n",
    "        \"AR PL UMing CN\",      # Another common CJK font\n",
    "        \"SimHei\",              # Windows\n",
    "        \"Microsoft YaHei\",     # Windows\n",
    "    ]\n",
    "    \n",
    "    # Get system font names\n",
    "    system_fonts = set(f.name for f in font_manager.fontManager.ttflist)\n",
    "    \n",
    "    for font_name in preferred_fonts:\n",
    "        if font_name in system_fonts:\n",
    "            return font_manager.FontProperties(family=font_name)\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        \"No suitable CJK font found. Please install 'fonts-noto-cjk' or another CJK-capable font.\"\n",
    "    )\n",
    "\n",
    "# Pick font with fallback\n",
    "cjk_font = get_font_with_cjk_fallback()\n",
    "\n",
    "\n",
    "def display_transcriptions_isolated(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "\n",
    "    if not(det_image is None or len(det_image) == 0):\n",
    "        # plot the groups\n",
    "        for ii, group in enumerate(det_image):\n",
    "            # draw boxes for each word in the group\n",
    "            for jj, word in enumerate(group):\n",
    "                # pick a different color for each word (we don't overlay GT because 2 transcriptions for each word would be a mess)\n",
    "                color = COLORS[(ii*jj) % len(COLORS)]\n",
    "                vertices = np.array(word[\"vertices\"])\n",
    "                edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "                ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "                # add the text\n",
    "                center = np.mean(vertices, axis=0)\n",
    "                ax.text(center[0], center[1], word[\"text\"], color=\"black\", fontsize=10, ha=\"center\", va=\"center\", fontproperties=cjk_font)\n",
    "\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_transcription_comparison_task3(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 3\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_transcriptions_isolated(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_transcriptions_isolated(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(3, OUTPUT_DIR_BASE, plot_transcription_comparison_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transcriptions_grouped(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if not(det_image is None or len(det_image) == 0):\n",
    "        # plot the groups\n",
    "        for ii, group in enumerate(det_image):\n",
    "            # pick a color for the group\n",
    "            color = COLORS[ii % len(COLORS)]\n",
    "            # draw lines between the words in the group\n",
    "            for i in range(1, len(group)):\n",
    "                vertices1 = np.array(group[i-1][\"vertices\"])\n",
    "                center1 = np.mean(vertices1, axis=0)\n",
    "                vertices2 = np.array(group[i][\"vertices\"])\n",
    "                center2 = np.mean(vertices2, axis=0)\n",
    "                ax.plot([center1[0], center2[0]], [center1[1], center2[1]], color=color, linewidth=2, alpha=0.8)\n",
    "            # draw boxes for each word in the group\n",
    "            for word in group:\n",
    "                vertices = np.array(word[\"vertices\"])\n",
    "                edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "                ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "                # add the text\n",
    "                center = np.mean(vertices, axis=0)\n",
    "                ax.text(center[0], center[1], word[\"text\"], color=\"black\", fontsize=10, ha=\"center\", va=\"center\", fontproperties=cjk_font)\n",
    "\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_transcription_comparison_task4(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 4\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "    # Ensure axs is always iterable\n",
    "    if len(valid_submissions)+1 == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_transcriptions_grouped(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_transcriptions_grouped(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(4, OUTPUT_DIR_BASE, plot_transcription_comparison_task4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra figure for the paper:\n",
    "2-rows figure for `rumsey/test/11792030_h3_w5.png` task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_extra_paper = f\"{OUTPUT_DIR_BASE}/extrafigs\"\n",
    "os.makedirs(outdir_extra_paper, exist_ok=True)\n",
    "for image_id in (\"rumsey/test/11792030_h3_w5.png\", \"rumsey/test/3287004_h2_w6.png\", ):\n",
    "    basename = os.path.splitext(os.path.basename(image_id))[0]\n",
    "    compare_submissions_to_gt_task1(\"rumsey\", image_id, filename=f\"{outdir_extra_paper}/{basename}.pdf\", use_two_rows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also for task 2 for control for the following images:\n",
    "- 9016007_h13_w9\n",
    "- 9103002_h2_w6\n",
    "- 9309000_h8_w11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir_extra_check = f\"{OUTPUT_DIR_BASE}/extracheck\"\n",
    "# os.makedirs(outdir_extra_check, exist_ok=True)\n",
    "# manual_selection_t2 = [\n",
    "#     \"rumsey/test/9016007_h13_w9.png\", \n",
    "#     \"rumsey/test/9103002_h2_w6.png\", \n",
    "#     \"rumsey/test/9309000_h8_w11.png\"]\n",
    "# for image_id in manual_selection_t2:\n",
    "#     basename = os.path.splitext(os.path.basename(image_id))[0]\n",
    "#     plot_gouping_comparison_task2(\"rumsey\", image_id, filename=f\"{outdir_extra_check}/{basename}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is an attempt to generate a plot task 3, sampling word images and displaying their transcriptions (cropping the word image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display some a random selecton of words (from random groups) from a submission\n",
    "# # show a crop of the image with the detected word and its transcription below\n",
    "# def display_word_sample(submission_indexed: dict[list[list]], image_id: str, word_count: int, submission_name=None, ax=None):\n",
    "#     # load the image\n",
    "#     image = open_image(image_id)\n",
    "#     image = np.array(image)\n",
    "\n",
    "#     # randomly select word_count words from all words from all groups in the submission\n",
    "#     rng = np.random.default_rng(seed=42)\n",
    "#     words = []\n",
    "#     for group in submission_indexed[image_id]:\n",
    "#         words.extend(group)\n",
    "#     word_count_ = min(word_count, len(words))\n",
    "#     words = rng.choice(words, size=word_count_, replace=False)\n",
    "\n",
    "#     # create a figure and axis\n",
    "#     # for each word, plot the word and the transcription below\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots(word_count, 1, figsize=(10, 10))\n",
    "#     for i, word in enumerate(words):\n",
    "#         vertices = np.array(word[\"vertices\"])\n",
    "#         x_min, y_min = np.min(vertices, axis=0).astype(int)\n",
    "#         x_max, y_max = np.max(vertices, axis=0).astype(int)\n",
    "#         # margin = 10\n",
    "#         # x_min = max(0, x_min - margin)\n",
    "#         # y_min = max(0, y_min - margin)\n",
    "#         # x_max = min(image.shape[1], x_max + margin)\n",
    "#         # y_max = min(image.shape[0], y_max + margin)\n",
    "#         # resize the image to a fixed size, respecting the aspect ratio\n",
    "#         image_crop = image[y_min:y_max, x_min:x_max]\n",
    "#         image_crop = Image.fromarray(image_crop)\n",
    "#         image_crop.thumbnail((128, 128))\n",
    "#         ax[i].imshow(image_crop)\n",
    "#         ax[i].axis(\"off\")\n",
    "#         ax[i].set_title(word[\"text\"])\n",
    "#         # increase title size\n",
    "#         ax[i].title.set_size(10)\n",
    "#     # if we have unused axes, hide them\n",
    "#     for i in range(word_count_, word_count):\n",
    "#         ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_image_comparison_detection(sample_submission_indexed, sample_gt_indexed, random_image_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_submissions_to_gt(task_id: int, subset: str, image_id: str):\n",
    "#     # load gt data\n",
    "#     gt = load_gt(subset)\n",
    "\n",
    "#     # list available submissions\n",
    "#     submissions_ids = list_valid_submissions(task_id, subset)\n",
    "#     # print(f\"Found {len(submissions_ids)} submissions\")\n",
    "#     # load all submissions and keep their ids\n",
    "#     submissions = {submission_id: load_submission(task_id, subset, submission_id) for submission_id in submissions_ids}\n",
    "#     # print(f\"Loaded {len(submissions)} submissions\")\n",
    "#     # index the ground truth\n",
    "#     gt_indexed = extract_detection_for_image(gt, image_id)\n",
    "#     # index the submissions\n",
    "#     submissions_indexed = {submission_id: index_detection_by_image(submission) for submission_id, submission in submissions.items()}\n",
    "    \n",
    "#     match task_id:\n",
    "#         case 3:\n",
    "#             # create a subplot with as many lines as there are submissions, plus one for the GT\n",
    "#             num_words = 10\n",
    "#             fig, axs = plt.subplots(len(submissions)+1, num_words, figsize=(num_words*5, (len(submissions)+1)*5))\n",
    "#             # display the ground truth\n",
    "#             display_word_sample(gt_indexed, image_id, num_words, submission_name=\"GT\", ax=axs[0])\n",
    "#             # display the submission title on the left of the first column\n",
    "#             axs[0, 0].set_ylabel(\"GT\")  # FIXME not displayed\n",
    "#             # for each submission, display a random selection of words\n",
    "#             for plot_id, (submission_id, submission_indexed) in enumerate(submissions_indexed.items()):\n",
    "#                 # retreive the submission name\n",
    "#                 submission_name = lookup_generate_title(int(submission_id), submissions_meta, user_to_team_name)\n",
    "#                 display_word_sample(submission_indexed, image_id, num_words, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "#                 # display the title one the left of the first column\n",
    "#                 axs[plot_id+1, 0].set_ylabel(submission_name)  # FIXME not displayed\n",
    "#             fig.suptitle(f\"Comparison of submissions to GT for image {image_id}\", fontsize=16)\n",
    "\n",
    "    \n",
    "#     # adjust layout\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # save the figure\n",
    "#     # TODO: save the figure to a file\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_submissions_to_gt(task_id=3, subset=\"ign\", image_id=random_image_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
