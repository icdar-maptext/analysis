{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images to investigate the predictions from each approach, without considering evaluation\n",
    "The idea is to produce a first, easier, set of images before taking more dev time to produce detailed visualization which re-run the evaluation selectively.\n",
    "\n",
    "We will try to compare predictions between methods and against the ground truth, for each task.\n",
    "\n",
    "We also want to be able to visualize:\n",
    "\n",
    "- best results (different for every method **OR, better, samples easy for all methods**)\n",
    "- worst results (different for every method **OR, better, samples hard for all methods**)\n",
    "- random results, with fixed seed (same input for every method, for comparison)\n",
    "\n",
    "For the RUMSEY subset, loading the predictions can be challenging because of the size of some files, the code developed for the IGN subset must be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check we have all images from test sets available.\n",
    "\n",
    "Note: generation of figures with images may be extracted to another notebook for clarity and to avoid the need to download image test sets for basic graph generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.loaders import check_for_missing_images, VALID_SUBSETS\n",
    "for subset in VALID_SUBSETS:\n",
    "    if len(missing_image := check_for_missing_images(subset)) > 0:\n",
    "        print(f\"Missing images in {subset}: {missing_image}\")\n",
    "        raise RuntimeError(f\"Missing images in {subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select some random images for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.loaders import list_gt_images, TypeDatasetName\n",
    "\n",
    "def select_random_images(subset: TypeDatasetName, n: int, seed: int = 42) -> list[str]:\n",
    "    images_list = list_gt_images(subset)\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(images_list)\n",
    "    return images_list[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the list of valid submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.loaders import load_valid_submissions_metadata\n",
    "\n",
    "valid_submissions_metadata = load_valid_submissions_metadata()\n",
    "valid_submissions_metadata.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.loaders import TypeTaskId\n",
    "import pandas as pd\n",
    "def select_valid_submissions(task_id: TypeTaskId, subset: TypeDatasetName) -> pd.DataFrame:\n",
    "    return valid_submissions_metadata[(valid_submissions_metadata.task == task_id) & (valid_submissions_metadata.subset == subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_valid_submissions(1, \"rumsey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display raw predictions for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a submission has the following format:\n",
    "# [ # Begin a list of images\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME1\",\n",
    "#      \"groups\": [ # Begin a list of phrase groups for the image\n",
    "#         [ # Begin a list of words for the phrase\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT1\"},\n",
    "#           ...,\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT2\"}\n",
    "#        ],\n",
    "#        ...\n",
    "#        [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT3}, ... ]\n",
    "#     ] },\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME2\",\n",
    "#      \"groups\": [\n",
    "#         [\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT4\"},\n",
    "#           ...,\n",
    "#           {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT5\"}\n",
    "#         ],\n",
    "#         ...\n",
    "#         [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT6\"}, ... ] \n",
    "#     ] },\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ground truth has the following format:\n",
    "# [ # Begin a list of images\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME1\",\n",
    "#      \"groups\": [ # Begin a list of phrase groups for the image\n",
    "#          [  # Begin a list of words for the phrase\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT1\", \"illegible\": False, \"truncated\": False},\n",
    "#            ...,\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT2\", \"illegible\": True, \"truncated\": False}\n",
    "#          ],\n",
    "#           ...\n",
    "#          [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT3\", \"illegible\": False, \"truncated\": True}, ... ]\n",
    "#      ] },\n",
    "#     {\n",
    "#      \"image\": \"IMAGE_NAME2\",\n",
    "#      \"groups\": [\n",
    "#          [\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT4\", \"illegible\": False, \"truncated\": False},\n",
    "#            ...,\n",
    "#            {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT5\", \"illegible\": False, \"truncated\": False}],\n",
    "#           ...\n",
    "#          [ {\"vertices\": [[x1, y1], [x2, y2], ..., [xN, yN]], \"text\": \"TEXT6\", \"illegible\": False, \"truncated\": False}, ... ] \n",
    "#      ] },\n",
    "#      ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detection_for_image(submission: list[dict], image_id: str) -> list[list]:\n",
    "    \"\"\"Extract predictions for a given image from  a submission or gt\n",
    "\n",
    "        [ [GROUP1], [GROUP2], ... ],\n",
    "    \"\"\"\n",
    "    for image_data in submission:\n",
    "        image_name = image_data[\"image\"]\n",
    "        if image_name == image_id:\n",
    "            return image_data[\"groups\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.loaders import open_image, load_gt, load_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_comparison_detection(gt_data_image: list[list[dict]], submission_data_image: list[list[dict]], image_id: str, submission_name=None, ax=None):\n",
    "    # TODO load the image in the caller\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # plot the ground truth\n",
    "    for group in gt_data_image:\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"red\" if (word[\"illegible\"] or word[\"truncated\"]) else \"green\"\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "\n",
    "    # plot the submission\n",
    "    for group in submission_data_image:\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=\"blue\", facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "    \n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    # title = f\"Predictions of {submission_name}\" if submission_name else \"Predictions\"\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    title += f\" vs GT\" #\\n{image_id}\"\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icdar24_maptext_analysis.submissions_metadata import shorten_title\n",
    "\n",
    "def compare_submissions_to_gt_task1(subset: str, image_id: str, filename: str = None, use_two_rows: bool = False):\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(1, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    _fig, axs = None, None\n",
    "    ax_row_len = None\n",
    "    if use_two_rows:\n",
    "        ax_row_len = (len(valid_submissions)+1)//2\n",
    "        _fig, axs = plt.subplots(2, ax_row_len, figsize=(5*ax_row_len, 5*2))\n",
    "    else:  # Use only 1 row\n",
    "        _fig, axs = plt.subplots(1, len(valid_submissions), figsize=(5*len(valid_submissions), 5))\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(1, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        plot_ax = axs[plot_id] if not use_two_rows else axs[plot_id//ax_row_len, plot_id%ax_row_len]\n",
    "        display_image_comparison_detection(gt_data_image, submission_data_image, image_id, submission_name=submission_name, ax=plot_ax)\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "\n",
    "        # enlarge text if we use two rows\n",
    "        if use_two_rows:\n",
    "            plot_ax.title.set_fontsize(20)\n",
    "\n",
    "    # add some extra vertical space between the two rows\n",
    "    if use_two_rows:\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # remove axis labels\n",
    "    if use_two_rows:\n",
    "        for ax in axs:\n",
    "            for axx in ax:\n",
    "                axx.axis(\"off\")\n",
    "                \n",
    "    else:\n",
    "        for ax in axs:\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average quality for each image over the submissions for a given task and subset\n",
    "from icdar24_maptext_analysis.loaders import load_results\n",
    "def compute_average_quality(task_id: int, subset: str) -> pd.Series:\n",
    "    valid_submission_ids = select_valid_submissions(task_id, subset).submission_id.to_list()\n",
    "    _results_global, results_per_image = load_results(task_id, subset, filter_fn=lambda x: int(x) in valid_submission_ids)\n",
    "    criterion = \"quality\"\n",
    "    if task_id == 4:\n",
    "        criterion = \"char_quality\"\n",
    "    return results_per_image[criterion].groupby(by=\"image_id\").mean().sort_values(ascending=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import os\n",
    "\n",
    "def generate_plot_raw_predictions(taskid: int, output_dir:str, display_function: Callable, num_images: int=10):\n",
    "    \"\"\"\n",
    "    display_function: (subset, image_id, filename) -> None\n",
    "    \"\"\"\n",
    "    output_dir = f\"{output_dir}/task{taskid}\"\n",
    "    # random images\n",
    "    for subset in reversed(VALID_SUBSETS):\n",
    "        out_dir = f\"{output_dir}/{subset}/random\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        random_images_subset = select_random_images(subset, num_images)\n",
    "        for ii, image_id in enumerate(random_images_subset):\n",
    "            print(f\"Generating plot for task {taskid}/{subset} {image_id} (random#{ii})\")\n",
    "            image_name = os.path.splitext(os.path.basename(image_id))[0]\n",
    "            display_function(subset, image_id, filename=f\"{out_dir}/{image_name}.pdf\")\n",
    "\n",
    "    # easy and hard images for current task\n",
    "    for subset in reversed(VALID_SUBSETS):\n",
    "        sample_sorted_values = compute_average_quality(taskid, subset)\n",
    "        easy_images = sample_sorted_values.head(num_images).index.to_list()\n",
    "        hard_images = sample_sorted_values.tail(num_images).index.to_list()\n",
    "        # generate comparison plots for the easy and hard images\n",
    "        for difficulty_name, difficulty_group in [(\"easy\", easy_images), (\"hard\", hard_images)]:\n",
    "            for ii, image_id in enumerate(difficulty_group):\n",
    "                print(f\"Generating plot for task {taskid}/{subset} {image_id} ({difficulty_name}#{ii})\")\n",
    "                image_name = os.path.splitext(os.path.basename(image_id))[0]\n",
    "                out_dir = f\"{output_dir}/{subset}/{difficulty_name}\"\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                display_function(subset, image_id, filename=f\"{out_dir}/{image_name}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and create the output directory\n",
    "OUTPUT_DIR_BASE = \"data/20-raw-predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results for task 1\n",
    "generate_plot_raw_predictions(1, OUTPUT_DIR_BASE, compare_submissions_to_gt_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display raw predictions for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "# FIXME instead of creating a complex and not really good colormap, just use:\n",
    "# - Paired: 12 colors\n",
    "# - Dark2: 8 colors\n",
    "# - Set1: 9 colors\n",
    "# - tab10: 10 colors\n",
    "\n",
    "# generate a set of N visually different colors which will be used to color the groups\n",
    "# colors should be visually distinct, but not too bright or too dark\n",
    "# picking them from a continuous color space seems a good idea\n",
    "# then, we shuffle them to avoid confusion between neighboring groups\n",
    "def generate_colors(n: int) -> list[tuple[float, float, float]]:\n",
    "    # generate a set of colors in the CIELAB color space\n",
    "    colors = plt.get_cmap(\"hsv\")\n",
    "    # colors = colors(range(n))\n",
    "    colors = colors(np.linspace(0, 1, n)) * 0.7 + 0.2  # make colors less dark and bright\n",
    "    # shuffle the colors to avoid confusion between neighboring groups\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    rng.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "# generate 32 colors\n",
    "n_colors = 32\n",
    "COLORS = generate_colors(n_colors)\n",
    "\n",
    "# plot the colormap\n",
    "fig, ax = plt.subplots(figsize=(10, 1))\n",
    "for i, color in enumerate(COLORS):\n",
    "    ax.add_patch(patches.Rectangle((i, 0), 1, 1, facecolor=color, edgecolor=\"none\"))\n",
    "ax.set_xlim(0, n_colors)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_groups(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # load the image\n",
    "    # if submission_name is not None:\n",
    "    #     print(f\"Displaying image {image_id} for submission {submission_name}\")\n",
    "    # else:\n",
    "    #     print(f\"Displaying image {image_id}\")\n",
    "\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # plot the groups\n",
    "    for ii, group in enumerate(det_image):\n",
    "        # pick a color for the group\n",
    "        color = COLORS[ii % len(COLORS)]\n",
    "        # draw boxes for each word in the group\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "        # draw lines between the words in the group\n",
    "        for i in range(1, len(group)):\n",
    "            vertices1 = np.array(group[i-1][\"vertices\"])\n",
    "            center1 = np.mean(vertices1, axis=0)\n",
    "            vertices2 = np.array(group[i][\"vertices\"])\n",
    "            center2 = np.mean(vertices2, axis=0)\n",
    "            ax.plot([center1[0], center2[0]], [center1[1], center2[1]], color=color, linewidth=2, alpha=0.8)\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gouping_comparison_task2(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 2\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_image_groups(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_image_groups(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(2, OUTPUT_DIR_BASE, plot_gouping_comparison_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transcriptions_isolated(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # plot the groups\n",
    "    for ii, group in enumerate(det_image):\n",
    "        # draw boxes for each word in the group\n",
    "        for jj, word in enumerate(group):\n",
    "            # pick a different color for each word (we don't overlay GT because 2 transcriptions for each word would be a mess)\n",
    "            color = COLORS[(ii*jj) % len(COLORS)]\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "            # add the text\n",
    "            center = np.mean(vertices, axis=0)\n",
    "            ax.text(center[0], center[1], word[\"text\"], color=\"black\", fontsize=10, ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_transcription_comparison_task3(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 3\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_transcriptions_isolated(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_transcriptions_isolated(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(3, OUTPUT_DIR_BASE, plot_transcription_comparison_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transcriptions_grouped(det_image: list[list], image: np.ndarray, submission_name=None, ax=None):\n",
    "    # create a figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # plot the groups\n",
    "    for ii, group in enumerate(det_image):\n",
    "        # pick a color for the group\n",
    "        color = COLORS[ii % len(COLORS)]\n",
    "        # draw lines between the words in the group\n",
    "        for i in range(1, len(group)):\n",
    "            vertices1 = np.array(group[i-1][\"vertices\"])\n",
    "            center1 = np.mean(vertices1, axis=0)\n",
    "            vertices2 = np.array(group[i][\"vertices\"])\n",
    "            center2 = np.mean(vertices2, axis=0)\n",
    "            ax.plot([center1[0], center2[0]], [center1[1], center2[1]], color=color, linewidth=2, alpha=0.8)\n",
    "        # draw boxes for each word in the group\n",
    "        for word in group:\n",
    "            vertices = np.array(word[\"vertices\"])\n",
    "            edgecolor = \"gray\" if (word.get(\"illegible\") or word.get(\"truncated\")) else color\n",
    "            ax.add_patch(patches.Polygon(vertices, edgecolor=edgecolor, facecolor=\"none\", linewidth=1, alpha=0.5))\n",
    "            # add the text\n",
    "            center = np.mean(vertices, axis=0)\n",
    "            ax.text(center[0], center[1], word[\"text\"], color=\"black\", fontsize=10, ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "    # ensure the aspect ratio is correct\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # ensure display boundaries match the image size\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    # set the title\n",
    "    title = f\"{submission_name}\" if submission_name else \"Predictions\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_transcription_comparison_task4(subset: str, image_id: str, filename:str =None):\n",
    "    TASK_ID = 4\n",
    "\n",
    "    # Load the image once for all\n",
    "    image = open_image(image_id)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # load gt data\n",
    "    gt_data = load_gt(subset)\n",
    "    gt_data_image = extract_detection_for_image(gt_data, image_id)\n",
    "\n",
    "    # load submissions metadata for the task and subset\n",
    "    valid_submissions = select_valid_submissions(TASK_ID, subset)\n",
    "\n",
    "    # create a subplot with as many columns as there are submissions\n",
    "    fig, axs = plt.subplots(1, len(valid_submissions)+1, figsize=(5*(len(valid_submissions)+1), 5))\n",
    "\n",
    "    # Display the ground truth\n",
    "    display_transcriptions_grouped(gt_data_image, image, submission_name=\"-- Ground Truth --\", ax=axs[0])\n",
    "\n",
    "    # for each submission, display the image with the ground truth and the submission, in the right subplot\n",
    "    for plot_id, (_row_id, row) in enumerate(valid_submissions.iterrows()):\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_data = load_submission(TASK_ID, subset, submission_id)\n",
    "        submission_data_image = extract_detection_for_image(submission_data, image_id)\n",
    "        submission_name = shorten_title(row[\"method_name\"])\n",
    "        display_transcriptions_grouped(submission_data_image, image, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "        # remove the submission data from memory\n",
    "        del submission_data\n",
    "        del submission_data_image\n",
    "    \n",
    "    # remove axis labels\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the figure\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # release the figure to avoid keeping it in memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plot_raw_predictions(4, OUTPUT_DIR_BASE, plot_transcription_comparison_task4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra figure for the paper:\n",
    "2-rows figure for `rumsey/test/11792030_h3_w5.png` task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_extra_paper = f\"{OUTPUT_DIR_BASE}/extrafigs\"\n",
    "os.makedirs(outdir_extra_paper, exist_ok=True)\n",
    "for image_id in (\"rumsey/test/11792030_h3_w5.png\", \"rumsey/test/3287004_h2_w6.png\", ):\n",
    "    basename = os.path.splitext(os.path.basename(image_id))[0]\n",
    "    compare_submissions_to_gt_task1(\"rumsey\", image_id, filename=f\"{outdir_extra_paper}/{basename}.pdf\", use_two_rows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also for task 2 for control for the following images:\n",
    "- 9016007_h13_w9\n",
    "- 9103002_h2_w6\n",
    "- 9309000_h8_w11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir_extra_check = f\"{OUTPUT_DIR_BASE}/extracheck\"\n",
    "# os.makedirs(outdir_extra_check, exist_ok=True)\n",
    "# manual_selection_t2 = [\n",
    "#     \"rumsey/test/9016007_h13_w9.png\", \n",
    "#     \"rumsey/test/9103002_h2_w6.png\", \n",
    "#     \"rumsey/test/9309000_h8_w11.png\"]\n",
    "# for image_id in manual_selection_t2:\n",
    "#     basename = os.path.splitext(os.path.basename(image_id))[0]\n",
    "#     plot_gouping_comparison_task2(\"rumsey\", image_id, filename=f\"{outdir_extra_check}/{basename}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is an attempt to generate a plot task 3, sampling word images and displaying their transcriptions (cropping the word image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display some a random selecton of words (from random groups) from a submission\n",
    "# # show a crop of the image with the detected word and its transcription below\n",
    "# def display_word_sample(submission_indexed: dict[list[list]], image_id: str, word_count: int, submission_name=None, ax=None):\n",
    "#     # load the image\n",
    "#     image = open_image(image_id)\n",
    "#     image = np.array(image)\n",
    "\n",
    "#     # randomly select word_count words from all words from all groups in the submission\n",
    "#     rng = np.random.default_rng(seed=42)\n",
    "#     words = []\n",
    "#     for group in submission_indexed[image_id]:\n",
    "#         words.extend(group)\n",
    "#     word_count_ = min(word_count, len(words))\n",
    "#     words = rng.choice(words, size=word_count_, replace=False)\n",
    "\n",
    "#     # create a figure and axis\n",
    "#     # for each word, plot the word and the transcription below\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots(word_count, 1, figsize=(10, 10))\n",
    "#     for i, word in enumerate(words):\n",
    "#         vertices = np.array(word[\"vertices\"])\n",
    "#         x_min, y_min = np.min(vertices, axis=0).astype(int)\n",
    "#         x_max, y_max = np.max(vertices, axis=0).astype(int)\n",
    "#         # margin = 10\n",
    "#         # x_min = max(0, x_min - margin)\n",
    "#         # y_min = max(0, y_min - margin)\n",
    "#         # x_max = min(image.shape[1], x_max + margin)\n",
    "#         # y_max = min(image.shape[0], y_max + margin)\n",
    "#         # resize the image to a fixed size, respecting the aspect ratio\n",
    "#         image_crop = image[y_min:y_max, x_min:x_max]\n",
    "#         image_crop = Image.fromarray(image_crop)\n",
    "#         image_crop.thumbnail((128, 128))\n",
    "#         ax[i].imshow(image_crop)\n",
    "#         ax[i].axis(\"off\")\n",
    "#         ax[i].set_title(word[\"text\"])\n",
    "#         # increase title size\n",
    "#         ax[i].title.set_size(10)\n",
    "#     # if we have unused axes, hide them\n",
    "#     for i in range(word_count_, word_count):\n",
    "#         ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_image_comparison_detection(sample_submission_indexed, sample_gt_indexed, random_image_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_submissions_to_gt(task_id: int, subset: str, image_id: str):\n",
    "#     # load gt data\n",
    "#     gt = load_gt(subset)\n",
    "\n",
    "#     # list available submissions\n",
    "#     submissions_ids = list_valid_submissions(task_id, subset)\n",
    "#     # print(f\"Found {len(submissions_ids)} submissions\")\n",
    "#     # load all submissions and keep their ids\n",
    "#     submissions = {submission_id: load_submission(task_id, subset, submission_id) for submission_id in submissions_ids}\n",
    "#     # print(f\"Loaded {len(submissions)} submissions\")\n",
    "#     # index the ground truth\n",
    "#     gt_indexed = extract_detection_for_image(gt, image_id)\n",
    "#     # index the submissions\n",
    "#     submissions_indexed = {submission_id: index_detection_by_image(submission) for submission_id, submission in submissions.items()}\n",
    "    \n",
    "#     match task_id:\n",
    "#         case 3:\n",
    "#             # create a subplot with as many lines as there are submissions, plus one for the GT\n",
    "#             num_words = 10\n",
    "#             fig, axs = plt.subplots(len(submissions)+1, num_words, figsize=(num_words*5, (len(submissions)+1)*5))\n",
    "#             # display the ground truth\n",
    "#             display_word_sample(gt_indexed, image_id, num_words, submission_name=\"GT\", ax=axs[0])\n",
    "#             # display the submission title on the left of the first column\n",
    "#             axs[0, 0].set_ylabel(\"GT\")  # FIXME not displayed\n",
    "#             # for each submission, display a random selection of words\n",
    "#             for plot_id, (submission_id, submission_indexed) in enumerate(submissions_indexed.items()):\n",
    "#                 # retreive the submission name\n",
    "#                 submission_name = lookup_generate_title(int(submission_id), submissions_meta, user_to_team_name)\n",
    "#                 display_word_sample(submission_indexed, image_id, num_words, submission_name=submission_name, ax=axs[plot_id+1])\n",
    "#                 # display the title one the left of the first column\n",
    "#                 axs[plot_id+1, 0].set_ylabel(submission_name)  # FIXME not displayed\n",
    "#             fig.suptitle(f\"Comparison of submissions to GT for image {image_id}\", fontsize=16)\n",
    "\n",
    "    \n",
    "#     # adjust layout\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # save the figure\n",
    "#     # TODO: save the figure to a file\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_submissions_to_gt(task_id=3, subset=\"ign\", image_id=random_image_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-TuuStEI5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
